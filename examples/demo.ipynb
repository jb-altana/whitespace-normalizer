{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if \"../\" not in sys.path: sys.path.append (\"../\")\n",
    "from modules import utils\n",
    "from modules.normalize import NGramsNormalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A factory method call to `NGramsNormalizer` can load the dictionaries from local files and create the `NGramsNormalizer` object. Note that this step consumes memory and can also take much time if the dictionaries are large like [Google Ngrams](http://storage.googleapis.com/books/ngrams/books/datasetsv2.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-06 14:18:46,013 INFO Unigrams loaded\n",
      "2019-03-06 14:19:51,277 INFO Bigrams loaded\n",
      "2019-03-06 14:24:45,720 INFO Trigrams loaded\n"
     ]
    }
   ],
   "source": [
    "grams = NGramsNormalizer.fromFiles (\"/hg191/corpora/google-ngrams/en.1M.filtered.1g\",\n",
    "                                    \"/hg191/corpora/google-ngrams/en.1M.filtered.2g\",\n",
    "                                    \"/hg191/corpora/google-ngrams/en.1M.filtered.3g\",\n",
    "                                    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to segment a few terms using different methods. The simplest method to use is `byLikelihoodRatio` which calculates the likelihood ratio using just the unigram and bigram statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "            \"aidand\", #aid and\n",
    "            \"often\", #often\n",
    "            \"themselvesdown\", #themselves down\n",
    "            \"Bespeak\", #Bespeak\n",
    "            \"Senatoradmits\", #Senator admits\n",
    "            \"Safeguard\" #Safeguard\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aid and\n",
      "often\n",
      "themselves down\n",
      "Bespeak\n",
      "Senator admits\n",
      "Safe guard\n"
     ]
    }
   ],
   "source": [
    "for example in examples:\n",
    "    print (grams.byLikelihoodRatio (example, smoothing=0.1, threshold=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output from the method looks fairly okay, though there can be false positives (eg. \"Safeguard\" is split to \"Safe guard\" when it shouldn't). Sometimes the surrounding context can help. The `byLikelihoodRatioContextual` method can make use of the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "        (\"our\", \"aidand\", \"our\"), #aid and\n",
    "        (\"memory\", \"often\", \"years\"),#of ten\n",
    "        (\"it\", \"often\", \"feels\"), #often\n",
    "        (\"let\", \"themselvesdown\", \"into\"), #themselves down\n",
    "        (\"cloud\", \"Bespeak\", \"the\"), #Bespeak\n",
    "        (\"The\", \"Senatoradmits\", \"that\"), #Senator admits\n",
    "        (\"a\", \"Safeguard\", \"against\") #Safeguard\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aid and\n",
      "of ten\n",
      "often\n",
      "themselves down\n",
      "Bespeak\n",
      "Senator admits\n",
      "Safeguard\n"
     ]
    }
   ],
   "source": [
    "for lc, example, rc in examples:\n",
    "    print (grams.byContextualLikelihoodRatio(example, lc, rc, interpolation=False, smoothing=0.1, threshold=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the extra context, the term \"Safeguard\" isn't incorrectly split anymore. Note also the example \"often\" which usually would remain as is without the context but is now rightly splits into \"of\" \"ten\" when it sees the span \"memory often years\" but keeps the original form \"often\" when it sees the span \"it often feels\". \n",
    "\n",
    "Internally, the method uses trigram and bigram statistics. Sometimes, trigram statistics can suffer from sparsity in which case we provide a way to compute interpolated probabilities and then calculate the likelihood ratios based on them. Simply turn on the `interpolation` flag to `True`. The interpolation weights for bigram and trigrams can be specified using `bigram_lambdas` and `trigram_lambdas` tuples respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aid and\n",
      "of ten\n",
      "often\n",
      "themselves down\n",
      "Bespeak\n",
      "Senator admits\n",
      "Safeguard\n"
     ]
    }
   ],
   "source": [
    "for lc, example, rc in examples:\n",
    "    print (grams.byContextualLikelihoodRatio(example, lc, rc, interpolation=True, smoothing=0.1, threshold=1, \n",
    "                                             bigram_lambdas=(0.9,0.1),\n",
    "                                             trigram_lambdas=(0.7, 0.2, 0.1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
